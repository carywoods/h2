# HarnessAI — The Mirror MVP
## Claude Code Build Prompt

You are building "The Mirror," the core product experience for HarnessAI, an operational intelligence firm. This is not a marketing site. It is a proof-of-competence engine that analyzes a prospect's business using public data and delivers a structured operational profile that demonstrates analytical capability.

## What This System Does

1. A visitor submits their company name, URL, and business email via a gated intake form.
2. The system verifies the email domain matches the submitted URL domain (or flags for manual review).
3. The system runs parallel public data collection against the submitted URL.
4. Aggregated raw data is sent to Claude Sonnet via the Anthropic API, which returns a structured operational profile in JSON.
5. The profile is stored and a private, expiring, token-authenticated link is sent to the visitor's email.
6. When the visitor opens the link, they see a staged visual assembly of their operational profile — panels appearing sequentially over 4-6 seconds.

## Architecture

Four Docker containers deployed via Coolify as a Docker Compose stack:

### 1. Frontend (`frontend`)
- Next.js or pure static HTML/JS/CSS — keep it simple
- Two views:
  - **Intake page**: Clean, minimal form. Three fields: Company Name, Company URL, Business Email. One submit button. No marketing copy beyond a single line: "Request your operational profile."
  - **Profile view**: Authenticated via URL token. Displays the structured operational profile with staged panel assembly animation.
- Visual design principles:
  - Near-zero decoration. No gradients, no rounded card corners, no shadow effects, no illustrations, no icons except functional ones.
  - One typeface: Inter. Three weights: Regular (data), Medium (labels), Semibold (company name and key metrics).
  - Size scale: 14px body, 12px metadata, 18px section headers, 28px company name.
  - One accent color: #1a2b4a (deep ink blue), used only on interactive elements and confidence indicators.
  - Monospaced numerals for all metric data.
  - Layout: Asymmetric grid. 65% left column (primary operational data), 35% right column (context, comparisons, data confidence).
- Animation spec:
  - Panel stagger: 300ms between panels appearing
  - Data within panels: 150ms sequential build
  - No fade-ins. Panels arrive with a clean translate-Y (20px → 0) + opacity (0 → 1) over 200ms ease-out.
  - Progress indicator during processing: horizontal rule extending left to right with small text labels as each data source completes.
  - After full assembly, page is completely static. No ambient animation.
- Single CTA appears only after full profile render: "This took seconds with public data. Imagine what we build with full access." Button: "Schedule a deep analysis." Links to a Calendly or equivalent.

### 2. API Orchestrator (`orchestrator`)
- Python FastAPI
- Endpoints:
  - `POST /intake` — receives form submission, validates email domain match against URL domain, queues analysis job, returns confirmation
  - `GET /profile/{token}` — returns profile JSON for authenticated token (tokens expire after 7 days)
  - `GET /status/{job_id}` — returns job status for internal monitoring
- On intake submission:
  1. Validate email domain matches URL domain (flexible matching — company.com URL accepts user@company.com, user@mail.company.com, etc.). Gmail/Yahoo/Outlook addresses flagged as "manual review" status rather than rejected.
  2. Generate unique job ID and auth token (UUID v4 for both).
  3. Dispatch parallel data collection tasks (async).
  4. Aggregate results from all workers.
  5. Send aggregated data to Anthropic API (Claude Sonnet) with structured output prompt.
  6. Validate returned JSON against profile schema.
  7. Store completed profile in Postgres.
  8. Send email with private link via email service.
- Timeout strategy: Each data collection worker has a 10-second timeout. If a worker fails or times out, the system proceeds with available data. The profile includes a "data confidence" section indicating which sources contributed.
- Rate limiting: Max 10 submissions per IP per hour via Redis.

### 3. Email Service
- Use Resend (https://resend.com) or fallback to SMTP relay
- Single transactional email template:
  - Subject: "Your HarnessAI Operational Profile"
  - Body: Minimal. "Your operational profile for [Company Name] is ready." + button linking to the authenticated profile URL. No marketing content.
- Environment variable for API key: `RESEND_API_KEY`

### 4. Redis (`redis`)
- Standard Redis container
- Used for: rate limiting (IP-based), job queue status, caching completed profiles (keyed by URL, TTL 24 hours so repeat requests don't re-run the full pipeline)

### 5. Postgres (`postgres`)
- Tables:
  - `submissions` — id, company_name, company_url, email, auth_token, status (queued/processing/complete/failed/manual_review), created_at, completed_at
  - `profiles` — id, submission_id, profile_json (JSONB), data_sources_used (array), confidence_score, created_at
  - `feedback` — id, profile_id, rating (1-5), comment (text), created_at

## Data Collection Workers

These are async Python functions within the orchestrator, not separate containers. Each returns structured data or a null/empty signal on failure.

### Worker 1: Site Scraper
- Fetch the target URL with httpx (async HTTP client) and parse with BeautifulSoup4
- Do NOT use Playwright or any headless browser — the container must stay lightweight for Coolify deployment
- Extract: page title, meta description, visible text content (first 5000 chars), navigation structure (menu items via <nav> and <a> tags), number of pages linked from homepage
- Follow up to 3 internal links (about page, services page, team page) if detected in navigation, to gather additional context
- Parse for: service/product descriptions, about page content, team size indicators, location mentions
- If the site is a JavaScript SPA that returns minimal HTML, note "JavaScript-rendered site — limited data from static analysis" in the output and proceed with whatever is available
- Return structured JSON

### Worker 2: Technology Detector
- Analyze HTTP response headers and HTML source of target URL
- Detect: CMS (WordPress, Shopify, Squarespace, custom), analytics tools (GA4, Mixpanel), payment processors, CDN, hosting indicators, JavaScript frameworks
- Use Wappalyzer-style rule matching (build a simplified version with 30-50 common technology signatures, not the full Wappalyzer database)
- Return array of detected technologies with confidence levels

### Worker 3: Google Business Profile
- Use Google Places API (Text Search) to find the business by name + location
- Extract: rating, review count, business category, hours, phone, address, photo count
- Environment variable: `GOOGLE_PLACES_API_KEY`
- Return structured JSON or null if not found

### Worker 4: DNS/WHOIS
- DNS lookup on the domain: MX records (email provider), TXT records (SPF/DKIM indicating email infrastructure maturity), nameservers (hosting provider indicator)
- WHOIS lookup: registration date (company age indicator), registrar
- Use python-whois and dnspython libraries
- Return structured JSON

### Worker 5: Job Posting Scanner
- Search Indeed or Google Jobs (via SerpAPI or similar) for "[company name] [location]" job postings
- Extract: number of open positions, job titles, departments hiring, seniority levels
- This worker is the most fragile — build it to gracefully return empty data if the search API is unavailable
- Environment variable: `SERPAPI_KEY` (optional — if not set, skip this worker)
- Return structured JSON or empty

## Anthropic API Integration

### Prompt for Profile Generation

Send the aggregated worker data to Claude Sonnet (`claude-sonnet-4-5-20250929`) with this system prompt:

```
You are an operational intelligence analyst. You receive raw data collected from public sources about a business. Your job is to produce a structured operational profile that demonstrates analytical depth and insight.

You must return ONLY valid JSON matching the schema below. No preamble, no markdown, no explanation outside the JSON.

Your analysis should:
- Draw non-obvious inferences from the data (e.g., MX records showing Google Workspace suggests cloud-forward operations; job postings for specific roles suggest strategic priorities)
- Identify operational strengths visible in the data
- Identify potential blind spots or areas where data suggests vulnerability
- Compare against general industry baselines where possible
- Be specific and grounded — never fabricate data points
- Note where confidence is low due to limited data

Profile JSON Schema:
{
  "company_name": "string",
  "industry_classification": "string",
  "location": "string",
  "estimated_size": "string (e.g., '10-50 employees', 'Solo operator', '50-200 employees')",
  "operational_snapshot": {
    "technology_posture": "string (2-3 sentence assessment of their technology stack and what it implies)",
    "digital_maturity": "string (1-10 rating with one-sentence justification)",
    "detected_technologies": ["array of strings"],
    "infrastructure_signals": "string (what DNS/hosting/email setup implies about operational sophistication)"
  },
  "market_position": {
    "business_category": "string",
    "public_reputation": "string (review data summary and what it implies)",
    "competitive_signals": "string (2-3 sentences on what the data suggests about their competitive position)",
    "growth_indicators": "string (hiring activity, web presence expansion, etc.)"
  },
  "strategic_observations": [
    "string (3-5 non-obvious observations drawn from the data, each 1-2 sentences)"
  ],
  "identified_gaps": [
    "string (2-3 areas where deeper analysis would reveal important insights, framed as opportunities not criticisms)"
  ],
  "data_confidence": {
    "overall_score": "string (High/Medium/Low)",
    "sources_used": ["array of source names"],
    "sources_unavailable": ["array of source names that returned no data"],
    "freshness": "string (e.g., 'Data collected February 2026')"
  }
}
```

### API Call Configuration
- Model: `claude-sonnet-4-5-20250929`
- Max tokens: 2500
- Temperature: 0.3 (we want consistency, not creativity)
- Environment variable: `ANTHROPIC_API_KEY`
- Retry: 3 attempts with exponential backoff (1s, 4s, 16s)
- If all retries fail, mark submission as "failed" and send an error notification email

## Docker Compose Structure (Coolify-Compatible)

This stack is designed for deployment via Coolify. Coolify manages Docker Compose natively — point it at the Git repo and it handles builds, networking, SSL, and restarts.

Key Coolify compatibility notes:
- Do NOT expose ports for postgres or redis — Coolify's internal Docker network handles service-to-service communication.
- The frontend and orchestrator expose ports only so Coolify's Traefik proxy can route to them.
- Environment variables should be set in Coolify's UI per-service, NOT in a .env file committed to the repo. The .env template below is for reference only.
- Coolify assigns internal hostnames to services based on the service name in the Compose file. Use these in DATABASE_URL and REDIS_URL.
- Do NOT use `depends_on` with `condition: service_healthy` — Coolify handles orchestration. Simple `depends_on` for startup ordering is fine.
- All volumes use named volumes so Coolify persists data across redeployments.

```yaml
services:
  frontend:
    build: ./frontend
    ports:
      - "3000:3000"
    environment:
      - NEXT_PUBLIC_API_URL=http://orchestrator:8000
    depends_on:
      - orchestrator
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000"]
      interval: 30s
      timeout: 5s
      retries: 3

  orchestrator:
    build: ./orchestrator
    ports:
      - "8000:8000"
    environment:
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      - GOOGLE_PLACES_API_KEY=${GOOGLE_PLACES_API_KEY}
      - RESEND_API_KEY=${RESEND_API_KEY}
      - SERPAPI_KEY=${SERPAPI_KEY}
      - DATABASE_URL=postgresql://harness:${DB_PASSWORD}@postgres:5432/harnessai
      - REDIS_URL=redis://redis:6379
      - BASE_URL=${BASE_URL}
    depends_on:
      - postgres
      - redis
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 5s
      retries: 3

  postgres:
    image: postgres:16-alpine
    environment:
      - POSTGRES_DB=harnessai
      - POSTGRES_USER=harness
      - POSTGRES_PASSWORD=${DB_PASSWORD}
    volumes:
      - pgdata:/var/lib/postgresql/data
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U harness -d harnessai"]
      interval: 10s
      timeout: 5s
      retries: 5

  redis:
    image: redis:7-alpine
    volumes:
      - redisdata:/data
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

volumes:
  pgdata:
  redisdata:
```

## Coolify Deployment Instructions

1. Push this project to a GitHub repo.
2. In Coolify, create a new Resource → Docker Compose. Point it at the repo.
3. Set environment variables in Coolify's UI for each service (see reference below). Do NOT commit secrets to the repo.
4. Assign a domain to the `frontend` service (e.g., app.harnessai.co). Coolify handles SSL via Let's Encrypt automatically.
5. Assign a separate domain or subdomain to the `orchestrator` service (e.g., api.harnessai.co) so the frontend can reach it from the browser. Alternatively, proxy /api/* from the frontend domain to the orchestrator.
6. Deploy. Coolify builds the Dockerfiles, pulls the Alpine images, wires the internal network, and starts everything.
7. Verify: hit the frontend domain, submit a test intake, confirm the orchestrator processes it and email is delivered.

Important: The `DATABASE_URL` and `REDIS_URL` use the Docker Compose service names (`postgres`, `redis`) as hostnames. Coolify's internal Docker network resolves these automatically. Do not replace them with localhost or external IPs.

## Orchestrator Requirements

The orchestrator Dockerfile should be lightweight. No headless browsers, no Chromium, no Playwright. The Python dependencies are:

```
fastapi
uvicorn[standard]
httpx
beautifulsoup4
lxml
python-whois
dnspython
anthropic
psycopg[binary]
redis
resend
pydantic
```

Base image: `python:3.12-slim`. Total image size should stay under 200MB.

## Environment Variables (Reference — set these in Coolify UI, not in a committed file)

```
ANTHROPIC_API_KEY=
GOOGLE_PLACES_API_KEY=
RESEND_API_KEY=
SERPAPI_KEY=
DB_PASSWORD=
BASE_URL=https://app.harnessai.co
```

## Build Order

1. Start with the orchestrator — get the FastAPI skeleton running with the `/intake` endpoint accepting form data and returning a job ID. Include a `GET /health` endpoint that returns `{"status": "ok"}` — Coolify uses this for health checks.
2. Build data collection workers one at a time. Start with Site Scraper (httpx + BeautifulSoup, most self-contained), then Technology Detector, then DNS/WHOIS, then Google Business, then Job Scanner last.
3. Build the Anthropic API integration — send test data, validate JSON output, implement retry logic.
4. Build the Postgres schema and storage layer.
5. Build the email delivery.
6. Build the frontend intake form.
7. Build the frontend profile view with staged assembly animation.
8. Wire everything together end-to-end.
9. Deploy to Coolify. Verify internal networking (orchestrator can reach postgres and redis by service name).
10. Test with 10 real Indiana business URLs.

## Critical Quality Gates

- The profile must never contain fabricated data. If the LLM hallucinates a data point not present in the worker output, the validation layer must catch it. Implement a post-generation check that verifies all "detected_technologies" entries came from the technology detector worker output, and all review data came from the Google Business worker output.
- If total data from all workers is below a minimum threshold (e.g., only DNS data available, everything else failed), do NOT generate a profile. Instead, mark as "insufficient data" and send a different email: "We need a bit more information to build your profile. Our team will follow up within 24 hours."
- The frontend must never show a partially loaded profile. Either the full profile renders with the staged animation, or the page shows a clean "Your profile is being prepared" message.

## What NOT to Build

- No user accounts or login system. Token-based URL access only.
- No admin dashboard for MVP. Monitor via direct database queries and application logs.
- No PDF export. The profile lives on the web only for now.
- No competitor comparison view (Phase 2).
- No strategic question decomposition feature (Phase 2).
- No payment processing. Sales conversations happen on calls.
